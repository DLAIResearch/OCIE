# OCIE:  Augmenting Model Interpretability with Explicit Objects and Explanations

Deep neural networks (DNNs) have become efficient tools for computer vision systems, delivering superior performance. However, their inherent black-box nature, along with bias and shortcut learning, raise concerns about their practical credibility. To address this issue, we propose OCIE, an interpretation enhancement algorithm that utilizes explicit object augmentation and explanations to enhance the reliability and interpretability of DNNs. Specifically, OCIE first processes the features learned by a self-supervised vision transformer using a graph-based algorithm to detect explicit objects in images. Then, class prototypes are constructed to exclude invalid detected objects. Finally, OCIE aligns explanations with explicit objects, ensuring that the model's attention is focused on the most discriminating features of objects in each category, rather than on irrelevant and frequently appearing backgrounds. Experimental results demonstrate that our approach enhances the explanation consistency of DNNs. Furthermore, our method improves the performance of models in fine-grained classification scenarios, especially the few-shot setting, both in terms of interpretation and classification. Our work also finds that more centralized explanations reduce the sufficiency of explanations for model decisions.

![Image image](https://github.com/DLAIResearch/OCIE/blob/main/misc/OCIE.jpg)
Code is being organized and will continue to be uploaded........
